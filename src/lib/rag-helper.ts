// src/lib/rag-helper.ts
import type { Env } from '../types';
import { retrieveChunks } from './rag';

export interface RAGResult {
  context: string;
  confidence: number;
  sources: Array<{ content: string; similarity: number; category?: string }>;
  hasContext: boolean;
}

/**
 * Hybrid RAG search with confidence scoring
 * Returns formatted context + confidence to decide if GPT fallback is needed
 */
export async function searchWithConfidence(
  env: Env, 
  query: string, 
  category?: string,
  topK: number = 5
): Promise<RAGResult> {
  console.log(`üîç [RAG] Buscando conhecimento interno: "${query.substring(0, 50)}..."`);
  
  try {
    const chunks = await retrieveChunks(env, query, topK);
    
    if (!chunks || chunks.length === 0) {
      console.log('‚ö†Ô∏è [RAG] Nenhum resultado encontrado na base');
      return {
        context: '',
        confidence: 0,
        sources: [],
        hasContext: false
      };
    }

    // Filter by category if specified
    let filteredChunks = chunks;
    if (category) {
      filteredChunks = chunks.filter((c: any) => c.category === category);
      console.log(`üìÇ [RAG] Filtrado por categoria "${category}": ${filteredChunks.length}/${chunks.length} chunks`);
    }

    // Calculate average similarity as confidence
    const avgSimilarity = filteredChunks.reduce((sum: number, c: any) => sum + (c.similarity || 0), 0) / filteredChunks.length;
    const confidence = Math.min(avgSimilarity, 1.0);

    // Format context for LLM
    const context = filteredChunks
      .map((c: any, i: number) => `[Fonte ${i + 1} - Similaridade: ${(c.similarity * 100).toFixed(0)}%]\n${c.content}`)
      .join('\n\n---\n\n');

    const sources = filteredChunks.map((c: any) => ({
      content: c.content,
      similarity: c.similarity,
      category: c.category
    }));

    console.log(`‚úÖ [RAG] ${filteredChunks.length} chunks encontrados (confian√ßa: ${(confidence * 100).toFixed(0)}%)`);

    return {
      context,
      confidence,
      sources,
      hasContext: filteredChunks.length > 0
    };
  } catch (error) {
    console.error('‚ùå [RAG] Erro na busca:', error);
    return {
      context: '',
      confidence: 0,
      sources: [],
      hasContext: false
    };
  }
}

/**
 * Enrich prompt with RAG context based on confidence thresholds
 */
export function enrichPrompt(basePrompt: string, ragResult: RAGResult): { prompt: string; mode: 'rag-only' | 'hybrid' | 'gpt-only' } {
  const { context, confidence, hasContext } = ragResult;

  // High confidence: use RAG primarily
  if (confidence >= 0.8 && hasContext) {
    console.log('üéØ [RAG] Alta confian√ßa - Modo RAG priorit√°rio');
    return {
      prompt: `${basePrompt}

CONHECIMENTO INTERNO (Alta Confian√ßa - ${(confidence * 100).toFixed(0)}%):
${context}

INSTRU√á√ïES:
- Priorize as informa√ß√µes do conhecimento interno acima
- Use seu conhecimento geral apenas para complementar
- Se houver conflito, prefira o conhecimento interno`,
      mode: 'rag-only'
    };
  }

  // Medium confidence: hybrid mode
  if (confidence >= 0.5 && hasContext) {
    console.log('‚öñÔ∏è [RAG] Confian√ßa m√©dia - Modo h√≠brido');
    return {
      prompt: `${basePrompt}

CONHECIMENTO INTERNO (Confian√ßa M√©dia - ${(confidence * 100).toFixed(0)}%):
${context}

INSTRU√á√ïES:
- Use as informa√ß√µes do conhecimento interno como refer√™ncia
- Complemente com seu conhecimento t√©cnico geral
- Valide e expanda as informa√ß√µes quando necess√°rio`,
      mode: 'hybrid'
    };
  }

  // Low/no confidence: GPT only
  console.log('ü§ñ [RAG] Baixa confian√ßa - Modo GPT puro');
  return {
    prompt: hasContext 
      ? `${basePrompt}

CONTEXTO DISPON√çVEL (Baixa Relev√¢ncia - ${(confidence * 100).toFixed(0)}%):
${context}

INSTRU√á√ïES:
- O contexto acima tem baixa relev√¢ncia
- Use principalmente seu conhecimento t√©cnico geral
- Apenas considere o contexto se for claramente aplic√°vel`
      : basePrompt,
    mode: 'gpt-only'
  };
}
